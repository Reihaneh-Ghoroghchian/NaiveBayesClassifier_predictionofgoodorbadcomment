{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Naive Bayes Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this project we try to classify a group of comments using naive bayes classifier. Using this classification, we will determine whether a comment is negative or positive. \n",
    "To do this, we find the probability of different features belonging to different classes. Using these probabilities, we find the probabilities of a new data belonging to each class. whichever class that has the highest probability, we choose as the class the new data belongs to."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1266,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "from numpy import random\n",
    "import math\n",
    "from tabulate import tabulate\n",
    "num_of_classes=2"
   ]
  },
  {
   "source": [
    "First we get the training data to train our model with."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1267,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_train = pd.read_csv('comment_train.csv').to_numpy()"
   ]
  },
  {
   "source": [
    "Part1. In this part, first we need to preprocess the data for the next parts. These preprocesses can be helpful or can reduce our accuracy. In what follows, we will see which ones help with our classification and decide which ones we will use on test data. \n",
    "We do these preprocesses using hazm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1269,
   "metadata": {},
   "outputs": [],
   "source": [
    "    comment_train1=comment_train.copy()\n",
    "    comment_train2=comment_train.copy()\n",
    "    comment_train3=comment_train.copy()\n",
    "    comment_train4=comment_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(comment_train1):\n",
    "    normalizer = Normalizer()\n",
    "    stemmer = Stemmer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    for i in range(len(comment_train1)):\n",
    "        for j in range(len(comment_train1[i])):\n",
    "            comment_train1[i][j]=normalizer.normalize(comment_train1[i][j])\n",
    "            comment_train1[i][j]=word_tokenize(comment_train1[i][j])\n",
    "            for k in range(len(comment_train1[i][j])):\n",
    "                comment_train1[i][j][k]=stemmer.stem(comment_train1[i][j][k])\n",
    "                comment_train1[i][j][k]=lemmatizer.lemmatize(comment_train1[i][j][k])\n",
    "    return comment_train1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_withjustlem(comment_train2):\n",
    "    normalizer = Normalizer()\n",
    "    lemmatizer = Lemmatizer()\n",
    "    for i in range(len(comment_train2)):\n",
    "        for j in range(len(comment_train2[i])):\n",
    "            comment_train2[i][j]=normalizer.normalize(comment_train2[i][j])\n",
    "            comment_train2[i][j]=word_tokenize(comment_train2[i][j])\n",
    "            for k in range(len(comment_train2[i][j])):\n",
    "                comment_train2[i][j][k]=lemmatizer.lemmatize(comment_train2[i][j][k])\n",
    "    return comment_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_withjuststem(comment_train3):\n",
    "    normalizer = Normalizer()\n",
    "    stemmer = Stemmer()\n",
    "    for i in range(len(comment_train3)):\n",
    "        for j in range(len(comment_train3[i])):\n",
    "            comment_train3[i][j]=normalizer.normalize(comment_train3[i][j])\n",
    "            comment_train3[i][j]=word_tokenize(comment_train3[i][j])\n",
    "            for k in range(len(comment_train3[i][j])):\n",
    "                comment_train3[i][j][k]=stemmer.stem(comment_train3[i][j][k])\n",
    "    return comment_train3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_nopre(comment_train4):\n",
    "    normalizer = Normalizer()\n",
    "    for i in range(len(comment_train4)):\n",
    "        for j in range(len(comment_train4[i])):\n",
    "            comment_train4[i][j]=normalizer.normalize(comment_train4[i][j])\n",
    "            comment_train4[i][j]=word_tokenize(comment_train4[i][j])\n",
    "    return comment_train4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1274,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_train_nopre=preprocess_nopre(comment_train4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1275,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['خیلی',\n",
       " 'بی',\n",
       " 'کیفیت',\n",
       " 'هست',\n",
       " 'و',\n",
       " 'سایزهاش',\n",
       " 'خیلی',\n",
       " 'کوچیک',\n",
       " 'و',\n",
       " 'بدون',\n",
       " 'کاربرد',\n",
       " 'هست',\n",
       " 'واقعا',\n",
       " 'من',\n",
       " 'توی',\n",
       " 'شگفت',\n",
       " 'انگیز',\n",
       " 'خریدم',\n",
       " 'ولی',\n",
       " 'تو',\n",
       " 'قیمت',\n",
       " 'شگفت',\n",
       " 'انگیز',\n",
       " 'هم',\n",
       " 'ارزش',\n",
       " 'خرید',\n",
       " 'نداره',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 1275
    }
   ],
   "source": [
    "comment_train_nopre[117][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1276,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_train_justlem=preprocess_withjustlem(comment_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1277,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['خیلی',\n",
       " 'بی',\n",
       " 'کیفیت',\n",
       " '#هست',\n",
       " 'و',\n",
       " 'سایز',\n",
       " 'خیلی',\n",
       " 'کوچیک',\n",
       " 'و',\n",
       " 'بدون',\n",
       " 'کاربرد',\n",
       " '#هست',\n",
       " 'واقعا',\n",
       " 'من',\n",
       " 'تو',\n",
       " 'شگفت',\n",
       " 'انگیز',\n",
       " 'خرید#خر',\n",
       " 'ولی',\n",
       " 'تو',\n",
       " 'قیمت',\n",
       " 'شگفت',\n",
       " 'انگیز',\n",
       " 'هم',\n",
       " 'ارزش',\n",
       " 'خرید',\n",
       " 'نداره',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 1277
    }
   ],
   "source": [
    "comment_train_justlem[117][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1278,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_train_juststem=preprocess_withjuststem(comment_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1279,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['خیل',\n",
       " 'ب',\n",
       " 'کیف',\n",
       " 'هس',\n",
       " 'و',\n",
       " 'سایز',\n",
       " 'خیل',\n",
       " 'کوچیک',\n",
       " 'و',\n",
       " 'بدون',\n",
       " 'کاربرد',\n",
       " 'هس',\n",
       " 'واقعا',\n",
       " 'من',\n",
       " 'تو',\n",
       " 'شگف',\n",
       " 'انگیز',\n",
       " 'خرید',\n",
       " 'ول',\n",
       " 'تو',\n",
       " 'قیم',\n",
       " 'شگف',\n",
       " 'انگیز',\n",
       " 'ه',\n",
       " 'ارز',\n",
       " 'خرید',\n",
       " 'نداره',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 1279
    }
   ],
   "source": [
    "comment_train_juststem[117][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_train = preprocess_all(comment_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1281,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['خیل',\n",
       " 'ب',\n",
       " 'کیف',\n",
       " 'هس',\n",
       " 'و',\n",
       " 'سایز',\n",
       " 'خیل',\n",
       " 'کوچیک',\n",
       " 'و',\n",
       " 'بدون',\n",
       " 'کاربرد',\n",
       " 'هس',\n",
       " 'واقعا',\n",
       " 'من',\n",
       " 'تو',\n",
       " 'شگف',\n",
       " 'انگیز',\n",
       " 'خرید',\n",
       " 'ول',\n",
       " 'تو',\n",
       " 'قیم',\n",
       " 'شگف',\n",
       " 'انگیز',\n",
       " 'ه',\n",
       " 'ارز',\n",
       " 'خرید',\n",
       " 'نداره',\n",
       " '.']"
      ]
     },
     "metadata": {},
     "execution_count": 1281
    }
   ],
   "source": [
    "comment_train[117][1]"
   ]
  },
  {
   "source": [
    "q1. Stemming helps us find a word despite it being plural or singular. In theory, this helps of prevent producing a lot of features that can make our classification slower. It also helps us have better accuracy. \n",
    "Lemmatization helps us find the root of a verb. In theory, this also help us with producing less features and having better accuracy by providing better probabilities.\n",
    "In what follows we will we whether this will be true. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Part 2. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this part, we use the second column for our calculations, since it has more words that can help us predict its category better."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "q2. Evidence are the information we already have from our training data. Since evidence is the same for all datas, we do not need to calculate it. It will not change the following comparison's results.\n",
    "Likelihood show us the probability of the test data blonging to a particular class. Since we are using naive bayes, this probability is the multiplications of the probabilities of each feature blonging to a class. \n",
    "Prior shows us the probability of a data belonging to a class based on the training data and without condidering the data's features.\n",
    "Post shows us the probability of a data belonging to a class based on the data's features and our classification model.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_numbers_in_features_table(s):\n",
    "    if s==['recommended']:\n",
    "        return 1\n",
    "    else :\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_feature_count(comment_train,num_of_classes):\n",
    "    features=[]\n",
    "    for i in range(num_of_classes+1):\n",
    "        features.append([])\n",
    "    tot_num=len(comment_train)\n",
    "    tot_recom=0\n",
    "    tot_not=0\n",
    "    for i in range(len(comment_train)):\n",
    "        if comment_train[i][2]==['recommended']:\n",
    "            tot_recom+=1\n",
    "        else:\n",
    "            tot_not+=1\n",
    "        for j in range(len(comment_train[i][1])):\n",
    "            if comment_train[i][1][j] in features[0]:\n",
    "                features[class_numbers_in_features_table(comment_train[i][2])][features[0].index(comment_train[i][1][j])]+=1\n",
    "            else:\n",
    "                features[0].append(comment_train[i][1][j])\n",
    "                features[1].append(0)\n",
    "                features[2].append(0)\n",
    "                features[class_numbers_in_features_table(comment_train[i][2])][features[0].index(comment_train[i][1][j])]+=1\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1284,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=cal_feature_count(comment_train,num_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1285,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "metadata": {},
     "execution_count": 1285
    }
   ],
   "source": [
    "features[1][1]"
   ]
  },
  {
   "source": [
    "q3. Since in naive bayes we multiply the probality of features in a class, if one of these probalities is zero, then the whole probality becomes zero and other features are ignored. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "q4. To prevent such happening, we use additive smoothing. We can add a constant like one to all the numbers of repeated features in a class. Hence, the features that does not exist in one class will not make the result zero and this will not change the outcome because adding the same constant to all the features does not change the result of comparison."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Part 3. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "q5. Precision shows us the number of detected recommnended to all the ones we assumed to be recommnended and recal shows us the number of detected recommnended to all the recommnendeds. \n",
    "If the precision is high, we know that among the ones we detected as recommnended most of them were correct, but there is a chance that there are a lot of recommnended ones that we did not detect and this will not be shown in the presicion. \n",
    "In contrast, if recall is high, we know that we detected a lot of recommnendeds but there is also the chance that we got a lot of not recommnendeds as recommnendeds, which again does not show in the recall.\n",
    "Hence, we need both precision and reacall to determine the quality of classification. If both of these numbers are high, it means that we were able to detect most of the recommnendeds and did not get a lot of wrongs."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "q6. Using this mean, the smaller the number is, the more its weight in calculating the mean becomes. If one of the numbers is very small, it effects the mean more than the other number does. In contrast, if we had used normal mean, if one of the numbers was very small but the other one very big, we could not have known that one of them was small which could bring the quality of classification down. Using this mean, the result will always show if one of the numbers is vert small, hence, we can accurately analyze our classification. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this part, first we get the test data, and then we decide based on the probabilities that we have, wether the data is recommended or not. Then we compare the predected results to the actual result to see how our classification works."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1286,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_test = pd.read_csv('comment_test.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1287,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'برد بالایی نداره کیفیت صداش معمولیه برای تایم تمرین و ورزش مناسبه از روی گوش نمیفته  ظریف نیست میان نمونه های هم رده خودش خیلی خوب نیست'"
      ]
     },
     "metadata": {},
     "execution_count": 1287
    }
   ],
   "source": [
    "comment_test[119][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1288,
   "metadata": {},
   "outputs": [],
   "source": [
    "    comment_test2=comment_test.copy()\n",
    "    comment_test3=comment_test.copy()\n",
    "    comment_test4=comment_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1289,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_test=preprocess_all(comment_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['برد',\n",
       " 'بالا',\n",
       " 'نداره',\n",
       " 'کیف',\n",
       " 'صدا',\n",
       " 'معمولیه',\n",
       " 'برا',\n",
       " 'تا',\n",
       " 'تمرین',\n",
       " 'و',\n",
       " 'ورز',\n",
       " 'مناسبه',\n",
       " 'از',\n",
       " 'رو',\n",
       " 'گو',\n",
       " 'نمیفته',\n",
       " 'ظریف',\n",
       " 'نیس',\n",
       " 'م',\n",
       " 'نمونه',\n",
       " 'ه',\n",
       " 'رده',\n",
       " 'خود',\n",
       " 'خیل',\n",
       " 'خوب',\n",
       " 'نیس']"
      ]
     },
     "metadata": {},
     "execution_count": 1290
    }
   ],
   "source": [
    "comment_test[119][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1291,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_test_justlem=preprocess_withjustlem(comment_test2)\n",
    "comment_test_juststem=preprocess_withjuststem(comment_test3)\n",
    "comment_test_nopre=preprocess_nopre(comment_test4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1292,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_real_outputs=[]\n",
    "for i in range(len(comment_test)):\n",
    "    test_real_outputs.append(class_numbers_in_features_table(comment_test[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_denum(de_features):\n",
    "    de_num=[0,0]\n",
    "    for i in range(len(de_features[0])):\n",
    "        for j in range(len(de_num)):\n",
    "            de_num[j]+=de_features[j+1][i]\n",
    "    return de_num\n",
    "# In this fuction we calculate the denum of the probability we the sum of all the words in that class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoothing(to_be_smoothed_features):\n",
    "    for i in range(len(to_be_smoothed_features[0])):\n",
    "        to_be_smoothed_features[1][i]+=1\n",
    "        to_be_smoothed_features[2][i]+=1\n",
    "    return to_be_smoothed_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_prob_of_features(train_features_num,denums):\n",
    "    for i in range(len(train_features_num)-1):\n",
    "        for j in range(len(train_features_num[i])):\n",
    "            train_features_num[i+1][j]=int(train_features_num[i+1][j])/int(denums[i])\n",
    "    return train_features_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_prob_of_single_testdata_log(test_features, prob_of_feaures):\n",
    "    prob_of_single_data=[0 ,0]\n",
    "    for i in range(len(test_features)):\n",
    "        if test_features[i] in prob_of_feaures[0]:\n",
    "            index=prob_of_feaures[0].index(test_features[i])\n",
    "            prob_of_single_data[0]+=math.log(prob_of_feaures[1][index])\n",
    "            prob_of_single_data[1]+=math.log(prob_of_feaures[2][index])\n",
    "    return prob_of_single_data  #returns 2 numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_prob_of_single_testdata_mul(test_features, prob_of_feaures):\n",
    "    prob_of_single_data=[1 ,1]\n",
    "    for i in range(len(test_features)):\n",
    "        if test_features[i] in prob_of_feaures[0]:\n",
    "            index=prob_of_feaures[0].index(test_features[i])\n",
    "            prob_of_single_data[0]*=prob_of_feaures[1][index]\n",
    "            prob_of_single_data[1]*=prob_of_feaures[2][index]\n",
    "    return prob_of_single_data  #returns 2 numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_output_withsmooth(test, train_features):\n",
    "    train_features = smoothing(train_features)\n",
    "    denums = cal_denum(train_features)\n",
    "    prob_of_feaures = cal_prob_of_features(train_features,denums)\n",
    "    predicted_out=[]\n",
    "    for i in range(len(test)):\n",
    "        single_test_prob=cal_prob_of_single_testdata_log(test[i][1], prob_of_feaures)\n",
    "        if single_test_prob[0]>single_test_prob[1]:\n",
    "            predicted_out.append(1)\n",
    "        else:\n",
    "            predicted_out.append(2)\n",
    "    return predicted_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calculated_out_test = cal_output_withsmooth(comment_test, features)\n",
    "out_to_see=calculated_out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_ac(calculated_out_test,test_real_outputs):\n",
    "    correct=0\n",
    "    for i in range(len(calculated_out_test)):\n",
    "        if calculated_out_test[i]==test_real_outputs[i]:\n",
    "            correct+=1\n",
    "    return(correct/len(calculated_out_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1301,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy= cal_ac(calculated_out_test,test_real_outputs)"
   ]
  },
  {
   "source": [
    "This accuracy is for both stemming and lemmatization were used.\n",
    "We also had smoothing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1302,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9025"
      ]
     },
     "metadata": {},
     "execution_count": 1302
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "source": [
    "More on 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1303,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy_lem 0.9125\n"
     ]
    }
   ],
   "source": [
    "features=cal_feature_count(comment_train_justlem,num_of_classes)\n",
    "calculated_out_test_lem = cal_output_withsmooth(comment_test_justlem, features)\n",
    "accuracy_lem= cal_ac(calculated_out_test_lem,test_real_outputs)\n",
    "print('accuracy_lem',accuracy_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1304,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy_stem 0.91\n"
     ]
    }
   ],
   "source": [
    "features=cal_feature_count(comment_train_juststem,num_of_classes)\n",
    "calculated_out_test_stem = cal_output_withsmooth(comment_test_juststem, features)\n",
    "accuracy_stem= cal_ac(calculated_out_test_stem,test_real_outputs)\n",
    "print('accuracy_stem',accuracy_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1305,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy_nopre 0.91375\n"
     ]
    }
   ],
   "source": [
    "features=cal_feature_count(comment_train_nopre,num_of_classes)\n",
    "calculated_out_test_nopre = cal_output_withsmooth(comment_test_nopre, features)\n",
    "accuracy_nopre= cal_ac(calculated_out_test_nopre,test_real_outputs)\n",
    "print('accuracy_nopre',accuracy_nopre)"
   ]
  },
  {
   "source": [
    "continue 1. With having the results, we can see that just doing normalization and tokenization we have the best accuracy compared to when we use stemming and lamminizaion. This is because "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(out_test,test_real_outputs):\n",
    "    correct_detected_recommended=0\n",
    "    all_detected_recommended=0\n",
    "    for i in range(len(out_test)):\n",
    "        if out_test[i]==1:\n",
    "            all_detected_recommended+=1\n",
    "        if (out_test[i]==test_real_outputs[i]) and (out_test[i]==1):\n",
    "            correct_detected_recommended+=1\n",
    "    return(correct_detected_recommended/all_detected_recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(_out_test,test_real_outputs):\n",
    "    correct_detected_recommended=0\n",
    "    all_recommended=0\n",
    "    for i in range(len(_out_test)):\n",
    "        if test_real_outputs[i]==1:\n",
    "            all_recommended+=1\n",
    "        if (_out_test[i]==test_real_outputs[i]) and (_out_test[i]==1):\n",
    "            correct_detected_recommended+=1\n",
    "    return (correct_detected_recommended/all_recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(a,b):\n",
    "    return (2*a*b)/(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1309,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy with pre with smooth 0.9025\n0.8815165876777251\n0.93\nf1_withpre_withsmooth 0.8623853211009176\n"
     ]
    }
   ],
   "source": [
    "print('accuracy with pre with smooth',accuracy)\n",
    "preps=precision(calculated_out_test,test_real_outputs)\n",
    "print(preps)\n",
    "reps=recall(calculated_out_test,test_real_outputs)\n",
    "print(reps)\n",
    "f1_withpre_withsmooth=f1(reps,preps)\n",
    "print('f1_withpre_withsmooth',f1_withs_withpre_withsmooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_output_withoutsmooth(test, train_features):\n",
    "    denums = cal_denum(train_features)\n",
    "    prob_of_feaures = cal_prob_of_features(train_features,denums)\n",
    "    predicted_out=[]\n",
    "    for i in range(len(test)):\n",
    "        single_test_prob=cal_prob_of_single_testdata_mul(test[i][1], prob_of_feaures)\n",
    "        if single_test_prob[0]>single_test_prob[1]:\n",
    "            predicted_out.append(1)\n",
    "        else:\n",
    "            predicted_out.append(2)\n",
    "    return predicted_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1311,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_pre_nosmooth=cal_feature_count(comment_train,num_of_classes)\n",
    "calculated_out_test_pre_nosmooth = cal_output_withoutsmooth(comment_test, features_pre_nosmooth)\n",
    "accuracy_withoutsmooth= cal_ac(calculated_out_test_pre_nosmooth,test_real_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1312,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy with pre no smooth 0.87125\n0.8938992042440318\n0.8425\nf1_withpre_withsmooth 0.8674388674388674\n"
     ]
    }
   ],
   "source": [
    "print('accuracy with pre no smooth',accuracy_withoutsmooth)\n",
    "prep=precision(calculated_out_test_pre_nosmooth,test_real_outputs)\n",
    "print(prep)\n",
    "rep=recall(calculated_out_test_pre_nosmooth,test_real_outputs)\n",
    "print(rep)\n",
    "f1_withpre_nosmooth=f1(rep,prep)\n",
    "print('f1_withpre_withsmooth',f1_withpre_nosmooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy without pre with smooth 0.91375\n0.902676399026764\n0.9275\nf1_withoutpre_withsmooth 0.9149198520345253\n"
     ]
    }
   ],
   "source": [
    "print('accuracy without pre with smooth',accuracy_nopre)\n",
    "pres=precision(calculated_out_test_nopre,test_real_outputs)\n",
    "print(pres)\n",
    "res=recall(calculated_out_test_nopre,test_real_outputs)\n",
    "print(res)\n",
    "f1_withoutpre_withsmooth=f1(res,pres)\n",
    "print('f1_withoutpre_withsmooth',f1_withoutpre_withsmooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1314,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=cal_feature_count(comment_train_nopre,num_of_classes)\n",
    "calculated_out_test_nopre_nosmooth = cal_output_withoutsmooth(comment_test_nopre, features)\n",
    "accuracy_nopre_nosmooth= cal_ac(calculated_out_test_nopre_nosmooth,test_real_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1315,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy without pre without smooth 0.86875\n0.90633608815427\n0.8225\nf1_withoutpre_withoutsmooth 0.8623853211009176\n"
     ]
    }
   ],
   "source": [
    "print('accuracy without pre without smooth',accuracy_nopre_nosmooth)\n",
    "pre=precision(calculated_out_test_nopre_nosmooth,test_real_outputs)\n",
    "print(pre)\n",
    "re=recall(calculated_out_test_nopre_nosmooth,test_real_outputs)\n",
    "print(re)\n",
    "f1_withoutpre_withoutsmooth=f1(re,pre)\n",
    "print('f1_withoutpre_withoutsmooth',f1_withoutpre_withoutsmooth)"
   ]
  },
  {
   "source": [
    "q7."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy Table\n                     with preprocess    without preprocess\n-----------------  -----------------  --------------------\nwith smoothing               0.9025                0.91375\nwithout smoothing            0.87125               0.86875\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy Table')\n",
    "print(tabulate([['with smoothing', str(accuracy),str(accuracy_nopre)], ['without smoothing', str(accuracy_withoutsmooth),str(accuracy_nopre_nosmooth)]], headers=['','with preprocess', 'without preprocess']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision Table\n                     with preprocess    without preprocess\n-----------------  -----------------  --------------------\nwith smoothing              0.881517              0.902676\nwithout smoothing           0.893899              0.906336\n"
     ]
    }
   ],
   "source": [
    "print('Precision Table')\n",
    "print(tabulate([['with smoothing', str(preps),str(pres)], ['without smoothing', str(prep),str(pre)]], headers=['','with preprocess', 'without preprocess']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1318,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recall Table\n                     with preprocess    without preprocess\n-----------------  -----------------  --------------------\nwith smoothing                0.93                  0.9275\nwithout smoothing             0.8425                0.8225\n"
     ]
    }
   ],
   "source": [
    "print('Recall Table')\n",
    "print(tabulate([['with smoothing', str(reps),str(res)], ['without smoothing', str(rep),str(re)]], headers=['','with preprocess', 'without preprocess']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1319,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 Table\n                     with preprocess    without preprocess\n-----------------  -----------------  --------------------\nwith smoothing              0.862385              0.91492\nwithout smoothing           0.867439              0.862385\n"
     ]
    }
   ],
   "source": [
    "print('F1 Table')\n",
    "print(tabulate([['with smoothing', str(f1_withs_withpre_withsmooth),str(f1_withoutpre_withsmooth)], ['without smoothing', str(f1_withpre_nosmooth),str(f1_withoutpre_withoutsmooth)]], headers=['','with preprocess', 'without preprocess']))"
   ]
  },
  {
   "source": [
    "q8. Considering accuracy, we can see that smoothing has a very big impact on the result. This is because(as explained before) when a word does not exist in either recommended or not recommended classes, it's probability becomes zero and after calculating the whole probability, it becomes zero and all the other features are ignored. Smoothing removes this problem.We can also see that the processes stemming and lemmitization has not help with the problem.\n",
    "Considering precision we can see that numbres are not very different from each other but over all not processed data has had better results because not processing helps us keep the exact words of each class which could help us not detect wrong ones, although we may miss a lot of right ones.\n",
    "Considering recal, we can see processing data and using smoothing creates the best results. Overall smoothing has a great impact on recall because smoothing prevents us from missing right answers, though we may choose a lot of wrong ones.\n",
    "Considering F1 which considers both precision and recall, we can see no preprocessing and using smoothing is the best solution, which we can see with accuracy too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "q9."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1320,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indexes=[]\n",
    "for i in range(len(calculated_out_test)):\n",
    "    if test_real_outputs[i]!=calculated_out_test[i]:\n",
    "        wrong_indexes.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: test_real_outputs 1\ncalculated_out_test 2\ncomment ['تازه', 'خرید', 'یه', 'مد', 'کار', 'بکنه', 'مشخص', 'میشه', 'کیف', 'قطعات']\n"
     ]
    }
   ],
   "source": [
    "index=0\n",
    "print('1:','test_real_outputs',test_real_outputs[wrong_indexes[index]])\n",
    "print('calculated_out_test',calculated_out_test[wrong_indexes[index]])\n",
    "print('comment',comment_test[wrong_indexes[index]][1])"
   ]
  },
  {
   "source": [
    "In this example the comment is recommended but we have guessed wrong. This is probably because words like کیفیت  , تاره ,کار ,etc are usually used when we want to say something is not working or does not have a good quality."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: test_real_outputs 1\ncalculated_out_test 2\ncomment ['سلا', '،', 'راح', 'شد#شو', 'از', 'کابل', 'شارژ', '،', 'توصیه', 'میشود', 'به', 'شد#شو', '.', 'ارز', 'گوش', 'خود', 'را', 'به', 'شارژ', 'وایرلس', 'مجهز', 'کرد#کن', '.']\n"
     ]
    }
   ],
   "source": [
    "index=1\n",
    "print('1:','test_real_outputs',test_real_outputs[wrong_indexes[index]])\n",
    "print('calculated_out_test',calculated_out_test[wrong_indexes[index]])\n",
    "print('comment',comment_test[wrong_indexes[index]][1])"
   ]
  },
  {
   "source": [
    "In this case also we have guessed a  recommended comment a not recommended one. This seems to be mainly because of using words like    ارز , توصیه, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: test_real_outputs 2\ncalculated_out_test 1\ncomment ['من', 'تو', 'تخفیف', 'ویژه', '۵', 'تا', 'خرید', 'و', 'همشون', 'رو', 'ه', 'تس', 'کرد#کن', 'و', 'اینکه', 'میگن', 'دوس', 'که', 'خرابه', 'برا', 'من', 'پ', 'نیومده', 'انتقال', 'فایل', 'ه', 'میشه', 'با', 'کامپیو', 'اون', 'مشکل', 'ندارن', 'شارژ', 'سریع', 'تس', 'کرد#کن', 'اون', 'اوک', 'هس', '(', 'دوس', 'اینکه', 'کابل', 'فقط', 'شار', 'سریع', 'باشه', 'کاف', 'نیس', 'برا', 'اینکه', 'گوشیتون', 'سریع', 'شارژ', 'بشه', ',', 'کلگ', 'شارژ', 'و', 'خود', 'گوش', 'ه', 'باید', 'قابل', 'شارژ', 'سریع', 'رو', 'ه', 'داشته', 'باشه', ')', 'فقط', 'تن', 'مشکل', 'اینکه', 'جنس', 'خود', 'س', 'یک', 'ب', 'کیف', 'هس', 'و', 'امیدوار', 'پاره', 'نشن', 'برا', 'مد', 'عمر', 'فقط', 'کوتاهس', 'بین', '۱', 'تا', '۳', 'ماه', 'ب', 'دوا', 'نداره', 'البته', 'یک', 'یک', 'هفته', 'ه', 'دوا', 'آورد#آور', 'در', 'ضمن', 'این', 'کابل', 'طرح', 'خوب', 'هستن', 'و', 'فابریک', 'سامسونگ', 'نیستن']\n"
     ]
    }
   ],
   "source": [
    "index=2\n",
    "print('1:','test_real_outputs',test_real_outputs[wrong_indexes[index]])\n",
    "print('calculated_out_test',calculated_out_test[wrong_indexes[index]])\n",
    "print('comment',comment_test[wrong_indexes[index]][1])"
   ]
  },
  {
   "source": [
    " Here we have guessed a not recommended as recommended probabily because of words like خوب  , فابریک,etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: test_real_outputs 1\ncalculated_out_test 2\ncomment ['فندک', 'قبل', 'مدا', 'فیوز', 'میسوزوند', 'و', 'یک', 'بار', 'شارژر', 'موبایل', 'ه', 'سوزوند', 'ول', 'با', 'این', 'هیچ', 'مشکل', 'بوجود', 'نیومده', 'تا', 'ال', '.', 'کیفیت', 'خیل', 'خوبه', 'و', 'لامپ', 'ه', 'داره']\n"
     ]
    }
   ],
   "source": [
    "index=3\n",
    "print('1:','test_real_outputs',test_real_outputs[wrong_indexes[index]])\n",
    "print('calculated_out_test',calculated_out_test[wrong_indexes[index]])\n",
    "print('comment',comment_test[wrong_indexes[index]][1])"
   ]
  },
  {
   "source": [
    "Use of words like نیومده , مشکل  is probabily the cause of confusion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1325,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1: test_real_outputs 2\ncalculated_out_test 1\ncomment ['جنس', 'و', 'چسبندگ', 'عالیه', 'اما', 'کل', 'صفحه', 'رو', 'نمیپوشونه', 'متاسفانه', '.', 'با', 'این', 'دید', 'اگر', 'بخرین', '،', 'ناراض', 'نمیشید', 'از', 'خریدتون', 'مثل', 'من', '.']\n"
     ]
    }
   ],
   "source": [
    "index=6\n",
    "print('1:','test_real_outputs',test_real_outputs[wrong_indexes[index]])\n",
    "print('calculated_out_test',calculated_out_test[wrong_indexes[index]])\n",
    "print('comment',comment_test[wrong_indexes[index]][1])"
   ]
  },
  {
   "source": [
    "Words like عالیه , بخرین  could be the cause of mistake. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}